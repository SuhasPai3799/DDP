\documentclass[a4paper]{memoir}

\usepackage[a4paper,margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[english]{babel}
\usepackage{multirow}
\usepackage{amsmath,graphicx}
\usepackage{hyperref}
\usepackage{fancyvrb}

\begin{document}

\title{Rudimant, the DFKI MLT dialogue engine}
\author{Bernd Kiefer, Anna Welker}
\date{\today}

\maketitle

\tableofcontents

\chapter{Purpose}

The multimodal interaction manager analyses natural language coming from the
user, and generates natural language and gestures for the robot resp. its
virtual replacement, the avatar. The generation is based on incoming stimuli,
like speech or text input, or high-level action requests coming from some
strategic planning component, or any other sensor input, if available.

The goal is to create engaging interactions with the users that support the
currently active high-level goals.

\chapter{Interaction with the overall system}

The interaction manager will get several input types from the nexus, the ones
currently foreseen are: input from automatic speech recognition (ASR) or typed
natural input, user parameters, like name, age, hobbies, etc. but also more
dynamic ones like mood or health data, and also triggers from high-level
planning.

All these inputs are stored as RDF data, based on an ontology developed as part
of the interaction manager, and available to all other components as a data
format specification.

When new data is added, a set of declaratively specified reactive rules will
propose dialogue moves or other actions and send these proposals to the action
selection mechanism. The selection mechanism eventually selects one of the
proposed actions and sends it back. If the proposed action results in dialogue
acts, these are turned into verbal output and gestures with the help of a
multimodal generation component, which retrieves parameters from the RDF
database to adapt the generation to the user's likings, and can also take into
account sensory data such as her or his estimated mood.

\section{Internal structure}

As shown in the picture below, the interaction manager consists of the RDF
store, which also contains the functionality to store incoming data in the
format specified by the ontology, thereby making it readily accessible for
other components.

The second major component is the rule processor for the dialogue management
rules, which generates proposals for actions when new incoming data
arrives. The rules not only use the new data, but also the interaction history
stored in the RDF database to take its decisions.

The last two parts are a robust natural language interpretation module (not
explicitly shown in the picture), which turns spoken or written utterances
into dialogue acts, possibly with an intermediate step that involves a more
elaborate semantic format, and a multimodal generation component, which turns
outgoing dialogue acts into natural language utterances and gestures.

\vspace*{4ex}

\includegraphics[width=.9\textwidth]{rudimant.png}

\input{design}

\chapter{Auspacken, Installation, Starten}

\input{hfc}

\input{system}

\end{document}
