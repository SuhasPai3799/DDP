\documentclass[a4paper]{memoir}

\usepackage[a4paper,margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[english]{babel}
\usepackage{multirow}
\usepackage{amsmath,graphicx}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage[textwidth=2.5cm, textsize=small]{todonotes}

\usepackage{xspace}
\newcommand{\vonda}{VOnDA\xspace}

\pgfdeclareimage[width=.99\columnwidth]{vondagui}{VondaGui}

\begin{document}

\title{\vonda, A Framework for Dialogue Management}
\author{Bernd Kiefer, Anna Welker}
\date{\today}

\maketitle

\tableofcontents

\chapter{Purpose}

\vonda is a framework to implement the dialogue management functionality in
dialogue systems. Although domain-independent, \vonda is tailored towards
dialogue systems with a focus on social communication, which implies the need
of a long-term memory and high user adaptivity. \vonda's specification and
memory layer relies upon (extended) RDF/OWL, which provides a universal and
uniform representation, and facilitates interoperability with external data
sources.

\vonda consists of three parts: A programming language tailored towards the
specification of reactive rules and transparent RDF data store usage, a
compiler that turns source code in this language into Java code, and a run-time
core, that supports implementing dialogue management modules using the compiled
rules.

The framework is domain-independent, it was originally designed to for
multi-modal human-robot interaction, but there is currently no \emph{special}
functionality in the core to either support the multi-modality nor the
human-robot interaction. The architecture of the framework is open and powerful
enough to add these things easily.

\chapter{Sketching a Simple Interaction Manager}

The simplest version of an interaction manager analyses natural language
coming from the user, and generates natural language and gestures for the robot
resp. its virtual replacement, the avatar. Generation is based on incoming
stimuli, like speech or text input, or high-level action requests coming from
some strategic planning component, or any other sensor input, if available.

The interaction manager will get several input types from the nexus, the ones
currently foreseen are: input from automatic speech recognition (ASR) or typed
natural input, user parameters, like name, age, hobbies, etc. but also more
dynamic ones like mood or health data, and also triggers from high-level
planning.

All these inputs are stored as RDF data, based on an ontology developed as part
of the interaction manager, and available to all other components as a data
format specification.

When new data is added, a set of declaratively specified reactive rules will
propose dialogue moves or other actions and send these proposals to the action
selection mechanism. The selection mechanism selects the ``best'' of the
proposed actions and sends it back. If the proposed action results in dialogue
acts, these are turned into verbal output and gestures with the help of a
multimodal generation component, which retrieves parameters from the RDF
database to adapt the generation to the user's likings, and can also take into
account sensory data such as her or his estimated mood.

\section{Internal structure}

As shown in the picture below, the interaction manager consists of the RDF
store, which also contains the functionality to store incoming data in the
format specified by the ontology, thereby making it readily accessible for
other components.

The second major component is the rule processor for the dialogue management
rules, which generates proposals for actions when new incoming data
arrives. The rules not only use the new data, but also the interaction history
stored in the RDF database to take its decisions.

The last two parts are a robust natural language interpretation module (not
explicitly shown in the picture), which turns spoken or written utterances
into dialogue acts, possibly with an intermediate step that involves a more
elaborate semantic format, and a multimodal generation component, which turns
outgoing dialogue acts into natural language utterances and gestures.

\vspace*{4ex}

\includegraphics[width=.9\textwidth]{rudimant.png}

%\input{design}

\chapter{Auspacken, Installation, Starten}

\input{hfc}

\input{system}

\end{document}
